---
title: "Tweets während der Konstituierung des 19.Bundestages"
#author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: github_document
---

Die Sitzung zur Konstituierung des 19.Bundestages fand am 24.10.2017 statt und konnte auf Phoenix live verfolgt werden. Twitter-User kommentierten die Sitzung unter dem Hashtag #Bundestag. Wir haben alle Tweets des Tages gesammelt die dieses Wort beinhalteten. 

Um einen Eindruck zu gewinnen, über welche Parteien was und wieviel getwittert wurde, generieren wir die Variable "Partei", die die jeweilige Nennung einer Partei im Tweet wiedergibt. 
Jeder Tweet wird also dahingehend bewertet, ob er eine Partei nennt. Es kann also vorkommen, dass Tweets mehrfach im Korpus auftauchen (Wenn mehr als eine Partei genannt wird.) Wenn keine Partei explizit genannt wird, dann bekommt der Tweet die Bezeichnung "None".

Weitere Erläuterungen zu den einzelnen Analyseschritten finden Sie [hier](https://franziloew.github.io/politsentiment/)

```{r, include=FALSE}
# load the packages
libs <- c("tidytext","tidyr","readr","lubridate","tm","stm",
          "plyr","dplyr","class","knitr","kableExtra","cldr",'network', 'sna', 'qdap',
          "htmlTable","ggplot2","gridExtra","jsonlite","stringr","scales")
lapply(libs, library, character.only = TRUE)
```

```{r, include=FALSE}
rm(list = ls())
load(file = "../../politsentiment_data/out/tweets_bundestag.Rda")

col <- c("deepskyblue1", "black","limegreen", "darkorchid2","gold","grey", "red2")
names(col) <- levels(tweets$partei)
```

### Zeitraum

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(tweets, aes(date, fill=partei)) + 
    geom_histogram(alpha = .8, adjust = .25, binwidth = 500) +
    scale_fill_manual(name = "", values = col) +
    xlab("") +
    ylab("Anzahl der Tweets") +
    scale_x_datetime(breaks = date_breaks("2 hour"), labels=date_format("%a-%d\n%H:%M", tz="CET"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# get the tweets that are retweeted 
tweets$isretweet <- ifelse(grepl('^RT', tweets$text),TRUE,FALSE)

# Extract the retweets
tweets %>%
  distinct(text,name, .keep_all = TRUE) %>%
  filter(isretweet == TRUE) -> rt

# pull the original author's name
rt$sender <- substr(rt$text, 5, regexpr(':', rt$text) - 1)

# Extract the retweets
orig <-  tweets[which(tweets$isretweet==FALSE),]
```

## Von welchen Plattformen werden die meisten Tweets gesendet?

```{r, echo=FALSE, message=FALSE}
tweets %>%
  group_by(platform_reduced) %>%
  tally() %>%
  ggplot(aes(reorder(platform_reduced,n), (n/1000))) +
  geom_col(fill = "blue", alpha = .7) +
  xlab("") +
  ylab("in tsd.") +
  coord_flip() 
```

## Wer retweeted wen?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
col3 = RColorBrewer::brewer.pal(3, 'Paired')

# Adjust retweets to create an edgelist for network
el <- as.data.frame(cbind(sender = tolower(rt$sender), 
                         receiver = tolower(rt$name)))
el <- count(el, sender, receiver) 
el <- el[which(el$n>1),]
el <- el[which(nchar(as.character(el$sender))>0),]
el <- el[which(nchar(as.character(el$receiver))>0),]

rtnet <- network(el, directed = TRUE, matrix.type = 'edgelist',
                ignore.eval = FALSE, names.eval = 'num')

# Get names of only those who were retweeted to keep labeling reasonable
vlabs <- rtnet %v% 'vertex.names'
vlabs[degree(rtnet, cmode = 'outdegree') < 15] = NA

par(mar = c(0, 0, 3, 0))
plot(rtnet, label = vlabs, label.col = "blue",
     label.pos = 5, label.cex = .8, 
     vertex.cex = log(degree(rtnet)) + .5, vertex.col = "gray",
     edge.lwd = 'num', edge.col = col3[3], main = '')
```

### Welche Tweets wurden am häufigsten Retweeted?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
orig$retweet_count <- as.numeric(orig$retweet_count)

orig %>%
  distinct(text, .keep_all = TRUE) %>%
  arrange(desc(retweet_count)) %>%
  select(name, text, retweet_count) %>%
  .[1:10,] %>%
  htmlTable(align = "l")
```

## Über welche Partei wird am meisten getweeted?

Anzahlt gesamte Tweets (ohne Retweets):
```{r, echo=FALSE}
htmlTable(nrow(orig))
```

```{r, echo=FALSE}
orig %>%
  #filter(source=="news") %>%
  ggplot(aes(partei))+
  geom_bar(fill = col, alpha = .8) +
  xlab("")+
  coord_flip()
```

### Nur Nachrichten-Accounts
```{r, include=FALSE}
# Filter news portals
news.names <- c("FOCUS Online","FOCUS Online TopNews","FOCUS Online Politik",
            "FAZ_NET komplett","FAZ Politik","FAZ Topthemen","FAZ.NET",
            "SPIEGEL ONLINE alles","SPIEGEL ONLINE","stern",
            "BILD","BILD Politik", "N24","ntv",
            "WELT","WELT Politik", "ZEIT ONLINE","ZEIT ONLINE Politik",
            "Handelsblatt","Handelsblatt Politik",
            "BuzzFeedNewsDE","BW Breaking News","taz","HuffPost Deutschland","MEEDIA", 
            "Der Tagesspiegel", "Süddeutsche Zeitung","Stuttgarter Zeitung","Hamburger Abendblatt","Westdeutsche Zeitung","FrankfurterRundschau","ZDF","ZDF heute","tagesschau","tagesthemen","Die Nachrichten","Deutschlandfunk","DW (Deutsch)","PHOENIX","WDR Aktuelle Stunde", "NDR", "NDR.de","NDR Info", "MDR", "MDR Aktuell")

# news sources
orig %>%
  mutate(source = ifelse(name %in% news.names, "news", "other")) ->orig
```

Anzahlt gesamte Tweets (ohne Retweets):
```{r, echo=FALSE}
  htmlTable(nrow(orig[which(orig$source=="news"),]))
```

```{r, echo=FALSE}
orig %>%
  filter(source=="news") %>%
  ggplot(aes(partei))+
  geom_bar(fill = c("deepskyblue1","black", "gold","grey","red2"), alpha = .8) +
  xlab("")+
  coord_flip()
```

## Term frequency

### Wordclouds

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(wordcloud)
# WordClouds ####
# get text
cdu <- orig[which(orig$partei=="CDU"),]
spd <- orig[which(orig$partei=="SPD"),]
fdp <- orig[which(orig$partei=="FDP"),]
gruene <- orig[which(orig$partei=="Die Grünen"),]
linke <- orig[which(orig$partei=="DIE LINKE"),]
afd <- orig[which(orig$partei=="AfD"),]

# Join texts in a vector 
cdu <- paste(cdu$text_cleaned, collapse = " ")
spd <- paste(spd$text_cleaned, collapse = " ")
fdp <- paste(fdp$text_cleaned, collapse = " ")
gruene <- paste(gruene$text_cleaned, collapse = " ")
linke <- paste(linke$text_cleaned, collapse = " ")
afd <- paste(afd$text_cleaned, collapse = " ")

# put everything in a single vector
all <- c(cdu, spd, fdp,gruene, linke, afd)

ignore <- paste(c("cdu","bündnis", "spd","fdp","gruene","grüne","grünen", "linke","dielinke","afd","gen","merkel","schulz","csu","union"), collapse = "|")
all <- gsub(ignore,"", all)

# Step 2: Corpus and term-docs_cleaned matrix ####
# create corpus
corp <- Corpus(VectorSource(all))

# create term-docs_cleaned matrix
tdm <- TermDocumentMatrix(corp)

# convert as matrix
tdm <- as.matrix(tdm)

# add column names
colnames(tdm) <- c("CDU","SPD","FDP","Die Grünen", "DIE LINKE","AfD")

# Step 3: Plot comparison wordcloud ####
comparison.cloud(tdm, random.order=FALSE, colors = c("black","red2", "gold","limegreen", "darkorchid2","deepskyblue1"), scale=c(3,.5),
                 title.size=1.2, max.words=150)
```


### Wordcloud (Tweets ohne Partei-Nennung)
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# get text
none <- orig[which(orig$partei=="None"),]

# Join texts in a vector 
none <- paste(none$text_cleaned, collapse = " ")

ignore <- paste(c("heute","schon","bundestag","geht","wer", "neuen","neue","mehr", "cdu","bündnis", "spd","fdp","gruene","grüne","grünen", "linke","dielinke","afd","gen","merkel","schulz","csu","union"), collapse = "|")
none <- gsub(ignore,"", none)

# Step 2: Corpus and term-docs_cleaned matrix ####
# create corpus
corp <- Corpus(VectorSource(none))

# create term-docs_cleaned matrix
tdm <- TermDocumentMatrix(corp)
# convert to matrix
tdm.m <- as.matrix(tdm)

ap.v <- sort(rowSums(tdm.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)

# Plot Wordcloud
pal2 <- brewer.pal(8,"Dark2")
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=5,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
```

### inverse document frequency (tf-idf)
 
```{r, include=FALSE}
token <- orig %>%
  group_by(partei) %>%
  unnest_tokens(word, text_cleaned) %>%
  count(partei, word, sort = TRUE)  %>%
  bind_tf_idf(word, partei, n) %>%
  arrange(desc(tf_idf))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ignore <- c("afd", "bundestag", "cdu","spd","fdp","linke","linken","dielinke","gruene", "grüne","grünen", "merkel","schulz","lindner","union","wagenknecht")

plot_tweets <- token %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

plot_tweets %>% 
  filter(!word %in% ignore) %>%
  top_n(5) %>%
  ggplot(aes(reorder(word, tf_idf), tf_idf, fill = partei)) +
  geom_col(alpha = .9, show.legend = FALSE) +
  scale_fill_manual(name="", values=col) +
  labs(x = NULL, y = "tf-idf") +
      facet_wrap(~partei, ncol = 2, scales = "free") +
  coord_flip()
```

### Bigrams
Welche zwei Wörter tauchen am häufigsten gemeinsam auf?
```{r, include=FALSE}
ignore <- c("afd", "bundestag", "cdu","spd","fdp","linke","linken","dielinke","gruene", "grüne","grünen", "merkel","schulz","lindner","union","wagenknecht")

bigrams <- orig %>%
  unnest_tokens(bigram, text_cleaned, token="ngrams", n=2)

bigrams_sep <- bigrams %>%
  separate(bigram, c("word1","word2"), sep = " ")

bigrams_sep <- bigrams_sep %>%
  filter(!word1 %in% ignore) %>%
  filter(!word2 %in% ignore)

bigrams <- bigrams_sep %>%
  unite(bigram, word1, word2, sep = " ")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot_tweets <- bigrams %>%
  count(bigram) %>%
  arrange(desc(n)) %>%
  .[1:20,] 

plot_tweets %>% 
  ggplot(aes(bigram, n)) +
  geom_col(fill="blue", alpha = .6) +
  scale_fill_manual(name="", values=col) +
  labs(x = NULL, y = "count") +
  coord_flip()
```

## Sentiment Analyse

```{r, message=FALSE, warning=FALSE, include=FALSE}
# Load dictionaries (from: http://wortschatz.uni-leipzig.de/de/download)
neg_df <- read_tsv("../dict/SentiWS_v1.8c_Negative.txt", col_names = FALSE)
names(neg_df) <- c("Wort_POS", "Wert", "Inflektionen")

neg_df %>% 
  mutate(Wort = str_sub(Wort_POS, 1, regexpr("\\|", .$Wort_POS)-1),
         POS = str_sub(Wort_POS, start = regexpr("\\|", .$Wort_POS)+1)) -> neg_df


pos_df <- read_tsv("../dict/SentiWS_v1.8c_Positive.txt", col_names = FALSE)
names(pos_df) <- c("Wort_POS", "Wert", "Inflektionen")

pos_df %>% 
  mutate(Wort = str_sub(Wort_POS, 1, regexpr("\\|", .$Wort_POS)-1),
         POS = str_sub(Wort_POS, start = regexpr("\\|", .$Wort_POS)+1)) -> pos_df

bind_rows("neg" = neg_df, "pos" = pos_df, .id = "neg_pos") -> sentiment_df

sentiment_df %>% select(neg_pos, Wort, Wert, Inflektionen, -Wort_POS) -> sentiment_df

sentiment_df %>% 
  rename(word = Wort) -> sentiment_df
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
token <- orig %>%
  unnest_tokens(word, text_cleaned)

sentiment_neg <- match(token$word, filter(sentiment_df, neg_pos == "neg")$word)
sentiment_pos <- match(token$word, filter(sentiment_df, neg_pos == "pos")$word)

token %>% 
  mutate(sentiment_neg = sentiment_neg,
         sentiment_pos = sentiment_pos) -> token

token %>%
  group_by(partei, word) %>%
  mutate(count = n()) -> token

token %>%
  group_by(partei) %>%
  filter(!is.na(sentiment_neg)) %>%
  dplyr::select(word, count) -> negative_sentiments

token %>%
  group_by(partei) %>%
  filter(!is.na(sentiment_pos)) %>%
  dplyr::select(word, count) -> positive_sentiments
```

#### Anzahl negativer Sentiment-Wörter

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot_sentiments <- negative_sentiments %>%
  arrange(desc(count)) %>%
  group_by(partei) %>%
  distinct() %>%
  mutate(word = factor(word, levels = rev(unique(word))))

plot_sentiments %>% 
  top_n(8) %>%
  ggplot(aes(reorder(word, count), count, fill = partei)) +
  geom_col(alpha = .8, show.legend = FALSE) +
  scale_fill_manual(values=col) +
  labs(x = NULL, y = "negative term count") +
  coord_flip()
```

#### Anzahl positiver Sentiment-Wörter

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot_sentiments <- positive_sentiments %>%
  arrange(desc(count)) %>%
  group_by(partei) %>%
  distinct() %>%
  mutate(word = factor(word, levels = rev(unique(word))))

plot_sentiments %>% 
  top_n(8) %>%
  ggplot(aes(reorder(word, count), count, fill = partei)) +
  geom_col(alpha = .9, show.legend = FALSE) +
  scale_fill_manual(values=col) +
  labs(x = NULL, y = "positive term count") +
     # facet_wrap(~partei, ncol = 2, scales = "free") +
  coord_flip()
```

### Gewichtete Analyse

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>% 
  left_join(sentiment_df, by = "word") -> token 

token %>% 
  group_by(partei) %>%
  filter(!partei=="None") %>%
  filter(!is.na(Wert)) %>% 
  summarise(Sentimentwert = sum(Wert, na.rm = TRUE)) %>%
  ggplot(aes(reorder(partei,Sentimentwert), Sentimentwert)) +
          xlab("") +
           geom_col(fill = c("deepskyblue1","darkorchid2","limegreen", "black","gold","red2"), alpha = .8)

```

## Was sind die Tweets mit den negativsten/positivsten Werten?

### CDU
```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "CDU") %>%
  arrange(Wert) %>%
    distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "CDU") %>%
  arrange(desc(Wert)) %>%
  distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

### SPD
```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "SPD") %>%
  arrange(Wert) %>%
    distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "SPD") %>%
  arrange(desc(Wert)) %>%
  distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

### FDP
```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "FDP") %>%
  arrange(Wert) %>%
    distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "FDP") %>%
  arrange(desc(Wert)) %>%
  distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

### AfD
```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "AfD") %>%
  arrange(Wert) %>%
    distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "AfD") %>%
  arrange(desc(Wert)) %>%
  distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

### Die Grünen
```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "Die Grünen") %>%
  arrange(Wert) %>%
    distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "Die Grünen") %>%
  arrange(desc(Wert)) %>%
  distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

### DIE LINKE
```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "DIE LINKE") %>%
  arrange(Wert) %>%
    distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "DIE LINKE") %>%
  arrange(desc(Wert)) %>%
  distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```
### Tweets ohne explizite Partei-Nennung
```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "None") %>%
  arrange(Wert) %>%
    distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
token %>%
  filter(partei == "None") %>%
  arrange(desc(Wert)) %>%
  distinct(text, .keep_all = TRUE) %>%
  select(text, Wert) %>%
  .[1:5,] %>%
  htmlTable(align="l")
```